# Periodontal Bone Loss Evaluation using Deep Learning-Based Image Segmentation

## ğŸ“¦ Project Structure

```
ğŸ“ scripts
â”œâ”€â”€ ğŸ“‚ tools
â”‚   â””â”€â”€ ğŸ“„ visualize.py     # Visualizes model predictions alongside ground-truth masks for qualitative analysis
â”œâ”€â”€ ğŸ“„ compare.py           # Compares predictions with ground-truth masks
â”œâ”€â”€ ğŸ“„ dcm2png.py           # Converts DICOM series into PNG slices
â”œâ”€â”€ ğŸ“„ download.py          # Downloads experiment logs from a remote server via SFTP
â”œâ”€â”€ ğŸ“„ evaluate.py          # Runs model evaluation for each fold, computes losses and metrics (e.g., mIoU)
â”œâ”€â”€ ğŸ“„ inference.py         # Generates segmentation masks by inference
â”œâ”€â”€ ğŸ“„ prepare_kfold.py     # Data splitting for K-Fold cross-validation
â”œâ”€â”€ ğŸ“„ run_experiment.py    # Main script to run complete experimental workflows
â””â”€â”€ ğŸ“„ train.py             # Entry point for single model training
ğŸ“ src
â”œâ”€â”€ ğŸ“‚ models
â”‚   â”œâ”€â”€ ğŸ“„ unet.py          # U-Net model architecture definition
â”‚   â””â”€â”€ ğŸ“„ ...              # Additional model implementation
â”œâ”€â”€ ğŸ“„ config.py            # Configuration settings and hyperparameters
â”œâ”€â”€ ğŸ“„ console.py           # Console output helpers (progress tracking and table-style summaries)
â”œâ”€â”€ ğŸ“„ dataset.py           # Custom Dataset and DataLoader implementation
â”œâ”€â”€ ğŸ“„ downloader.py        # Utilities for downloading experiment directories from a remote server using SFTP
â”œâ”€â”€ ğŸ“„ losses.py            # Custom loss functions
â”œâ”€â”€ ğŸ“„ metrics.py           # Evaluation metrics (e.g., mIoU)
â”œâ”€â”€ ğŸ“„ optimizers.py        # Optimizer construction utilities
â”œâ”€â”€ ğŸ“„ summary.py           # TensorBoard log parsing and scalar summary utilities
â””â”€â”€ ğŸ“„ trainer.py           # Handles training and validation loops
```

## ğŸ“ Dataset Preparation

The dataset must follow the directory structure below:
```
ğŸ“ datasets/<DATASET_NAME>
â”œâ”€â”€ ğŸ“‚ image
â”‚   â”œâ”€â”€ ğŸ“‚ data_1
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ 91.png
â”‚   â”‚   â””â”€â”€ ğŸ“„ ...
â”‚   â””â”€â”€ ğŸ“‚ ...
â””â”€â”€ ğŸ“‚ mask
    â”œâ”€â”€ ğŸ“‚ data_1
    â”‚   â”œâ”€â”€ ğŸ“„ 91.png
    â”‚   â””â”€â”€ ğŸ“„ ...
    â””â”€â”€ ğŸ“‚ ...
```
**Requirements**
* Images and masks must share identical folder/file names.
* Mask images must contain pixel labels `{0, 1, 2}` corresponding 3 classes.

### DICOM to PNG Conversion

You can use the following script to convert DICOM series into PNG slices:
```
python -m scripts.dcm2png <DICOM_DIR> datasets/<DATASET_NAME>/image
```
This command converts each DICOM series into slice-wise PNG images and saves them under:
```
datasets/<DATASET_NAME>/image/data_<ID>/*.png
```

### Inference (Generate Masks)

You can use a trained model checkpoint to perform inference-only segmentation:
```
python -m scripts.inference <EXPERIMENT_NAME> datasets/<DATASET_NAME>/image datasets/<DATASET_NAME>/mask --fold <FOLD>
```
Notes
* `--fold <FOLD>` is used to select the trained model checkpoint from logs/<EXPERIMENT_NAME>/Fold_<FOLD>/best.pth

## âš™ï¸ Configuration (`configs/config.toml`)

```toml=
# System and Experiment
experiment = "UNet_baseline"
seed = 42
num_workers = 4

# Data Configuration
datasets = ["bone_tooth_mask"]
split_filename = "bone_tooth_mask"
num_folds = 4
batch_size = 16

# Training Settings
num_epochs = 50

# Model Architecture
[model]
name = "UNet"

[model.parameters]
in_channels = 1
num_classes = 3

# Optimizer
[optimizer]
name = "Adam"

[optimizer.parameters]
lr = 1e-4

# Loss Function
[loss]
name = "MultipleLoss"
main_loss = "Total Loss"

[loss.parameters]
num_classes = 3

# Metric
[metric]
name = "mIoU"

[metric.parameters]
num_classes = 3
```
| You can modify this file or override parameters inside the scripts if needed.

## ğŸ”€ Generate K-Fold Split

```
python -m scripts.prepare_kfold
```
Generates:
```
splits/<SPLIT_FILENAME>.json
{
    "1": {
        "<DATASET_NAME_1>": [
            "data_1",
            ...
        ],
        "<DATASET_NAME_2>": [
            "data_1",
            ...
        ],
        ...
    },
    "2": {...},
    "3": {...},
    "4": {...}
}
```

## ğŸ‹ï¸ Train Model

**Train a single fold**

```
python -m scripts.train --fold 1
```

**Run full experiment (all folds)**

```
python -m scripts.run_experiment
```

Results saved as:
```
ğŸ“ logs/<EXPERIMENT_NAME>
â”œâ”€â”€ ğŸ“‚ Fold_1
â”‚   â”œâ”€â”€ ğŸ“„ best.pth
â”‚   â””â”€â”€ ğŸ“„ last.pth
â”œâ”€â”€ ğŸ“‚ Fold_2
â”œâ”€â”€ ğŸ“‚ Fold_3
â”œâ”€â”€ ğŸ“‚ Fold_4
â”œâ”€â”€ ğŸ“„ <SPLIT_FILENAME>.json
â””â”€â”€ ğŸ“„ config.toml
```

## ğŸ“Š Validation Model Performance

After training is completed, you can run validation to assess model performance on each fold:
```
python -m scripts.evaluate <EXPERIMENT_NAME>
```

This script automatically loads checkpoints for all folds and reports:
* Validation Loss
* Evaluation Metric (e.g., mIoU)

Example Console Output:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        â”‚ Total Loss â”‚ Dice Loss â”‚ Cross Entropy Loss â”‚   mIoU   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Fold 1 â”‚ 0.068619   â”‚ 0.130197  â”‚ 0.007042           â”‚ 0.949898 â”‚
â”‚ Fold 2 â”‚ 0.075404   â”‚ 0.146048  â”‚ 0.004760           â”‚ 0.948226 â”‚
â”‚ Fold 3 â”‚ 0.070967   â”‚ 0.133843  â”‚ 0.008092           â”‚ 0.936442 â”‚
â”‚ Fold 4 â”‚ 0.074820   â”‚ 0.145908  â”‚ 0.003733           â”‚ 0.950953 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” Compare Predictions with Ground Truth

```
python -m scripts.compare <EXPERIMENT_NAME>
```
Outputs:
```
ğŸ“ outputs/<EXPERIMENT_NAME>
â”œâ”€â”€ ğŸ“‚ Fold_1
â”‚   â”œâ”€â”€ ğŸ“‚ <DATASET_NAME_1>
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ data_1
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ compare
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ 91.png
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ ...
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ predict
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ 91.png
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ ...
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ ground_truth.npy
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ volume.npy
â”‚   â”‚   â””â”€â”€ ğŸ“‚ ...
â”‚   â””â”€â”€ ğŸ“‚ <DATASET_NAME_2>
â”œâ”€â”€ ğŸ“‚ Fold_2
â”œâ”€â”€ ğŸ“‚ Fold_3
â””â”€â”€ ğŸ“‚ Fold_4
```

## ğŸ‘ï¸ Visualize Predictions

You can visualize the segmentation predictions together with ground truth masks:
```
python -m scripts.tools.visualize <EXPERIMENT_NAME>
```

## ğŸ” Remote Server Connection

Download experiment logs from a remote server via SFTP.

Create `.env` from the example and fill in your credentials:
```
cp .env.example .env
```
```
SFTP_HOSTNAME = your.server.address
SFTP_PORT = 22
SFTP_USERNAME = your_username
SFTP_PASSWORD = your_password
```
```
python -m scripts.download <EXPERIMENT_NAME>
```

`.env` should not be committed and must be listed in `.gitignore`.

## ğŸ“ Notes

* Ensure the dataset follows the required structure.
* Modify `configs/config.toml` to customize model, loss, optimizer, and metrics.
* The framework is modular and extensible, allowing new models, loss functions, metrics, and optimizers to be added under `src/models/`, `src/losses.py`, `src/metrics.py`, and `src/optimizers.py`.
